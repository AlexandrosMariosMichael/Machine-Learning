{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 150000 # Set maximum number of steps to run environment.\n",
    "run_path = \"ppo\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 1000 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"SpaceShooter\" # Name of the training environment file.\n",
    "curriculum_file = \"Curriculum.json\"\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 64 # Number of units in hidden layer.\n",
    "batch_size = 32 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'SpaceShooterAcademy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: SpaceShooterAcademy\n",
      "        Number of brains: 2\n",
      "        Reset Parameters :\n",
      "\t\thazards -> 3.0\n",
      "\t\thazards_count -> 3.0\n",
      "Unity brain name: SpaceNavigatorBrain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 8\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 8\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: None, Up, Down, Left, Right, Fire at asteroid, Fire at alien ship, Fire at alien missile\n",
      "Unity brain name: SpaceShooterBrain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 8\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 3\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: Fire at asteroid, Fire at alien ship, Fire at alien missile\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000. Mean Reward: 3.3178841121320746. Std of Reward: 1.5464012694796956.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 1 : \thazards -> 2.0, hazards_count -> 3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2000. Mean Reward: 2.2536873410788765. Std of Reward: 1.372060356839031.\n",
      "Step: 3000. Mean Reward: 1.8222275349519084. Std of Reward: 1.888033940677971.\n",
      "Step: 4000. Mean Reward: 1.5715552846512. Std of Reward: 1.4101016654776415.\n",
      "Step: 5000. Mean Reward: 1.7874030037060595. Std of Reward: 1.8541378937066426.\n",
      "Step: 6000. Mean Reward: 1.5890502656440708. Std of Reward: 2.107685153808033.\n",
      "Step: 7000. Mean Reward: 2.3988473798367824. Std of Reward: 2.4006105261440527.\n",
      "Step: 8000. Mean Reward: 2.6602720273834803. Std of Reward: 2.1591999427358535.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 2 : \thazards -> 3.0, hazards_count -> 4.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 9000. Mean Reward: 3.6585142642127755. Std of Reward: 2.7667932438395346.\n",
      "Step: 10000. Mean Reward: 4.109208797199947. Std of Reward: 2.641480150420117.\n",
      "Step: 11000. Mean Reward: 3.4894162918074003. Std of Reward: 2.5395295986404016.\n",
      "Step: 12000. Mean Reward: 3.5514105708236796. Std of Reward: 2.673805874229244.\n",
      "Step: 13000. Mean Reward: 2.8792380291711708. Std of Reward: 2.6168118264824494.\n",
      "Step: 14000. Mean Reward: 3.399885445213277. Std of Reward: 3.1031291393419522.\n",
      "Step: 15000. Mean Reward: 3.616912027532659. Std of Reward: 3.5780362358389186.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 3 : \thazards -> 4.0, hazards_count -> 5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 16000. Mean Reward: 4.299830430202179. Std of Reward: 3.8106091330143763.\n",
      "Step: 17000. Mean Reward: 3.6004471170402588. Std of Reward: 2.706980087079334.\n",
      "Step: 18000. Mean Reward: 3.6388292294380786. Std of Reward: 3.2728340276038903.\n",
      "Step: 19000. Mean Reward: 4.096762620228291. Std of Reward: 3.75639584071365.\n",
      "Step: 20000. Mean Reward: 3.725777448621159. Std of Reward: 3.9442168615602218.\n",
      "Step: 21000. Mean Reward: 3.1904578732828672. Std of Reward: 3.2477297595453263.\n",
      "Step: 22000. Mean Reward: 3.5264093788959996. Std of Reward: 3.4150691357607137.\n",
      "Step: 23000. Mean Reward: 3.609510239298898. Std of Reward: 3.8333974183367823.\n",
      "Step: 24000. Mean Reward: 2.872189915938286. Std of Reward: 2.881692386392572.\n",
      "Step: 25000. Mean Reward: 3.00891196465376. Std of Reward: 2.770776997617334.\n",
      "Step: 26000. Mean Reward: 3.115161517759768. Std of Reward: 2.9050460038688373.\n",
      "Step: 27000. Mean Reward: 3.346004883181398. Std of Reward: 2.7989234677179753.\n",
      "Step: 28000. Mean Reward: 3.888734105304536. Std of Reward: 3.0470314530397764.\n",
      "Step: 29000. Mean Reward: 3.452898885603555. Std of Reward: 2.4192967496857647.\n",
      "Step: 30000. Mean Reward: 3.409827699910519. Std of Reward: 2.98383882141863.\n",
      "Step: 31000. Mean Reward: 3.5518827687977215. Std of Reward: 3.541970456231855.\n",
      "Step: 32000. Mean Reward: 4.066682354960159. Std of Reward: 3.5109195506148128.\n",
      "Step: 33000. Mean Reward: 4.417351142346554. Std of Reward: 3.9679050287914124.\n",
      "Step: 34000. Mean Reward: 4.3595384478660595. Std of Reward: 4.046076910606218.\n",
      "Step: 35000. Mean Reward: 4.053284003008179. Std of Reward: 3.8033128764385284.\n",
      "Step: 36000. Mean Reward: 4.161678124085519. Std of Reward: 3.9480811310781303.\n",
      "Step: 37000. Mean Reward: 3.8218228423526193. Std of Reward: 4.225584181225071.\n",
      "Step: 38000. Mean Reward: 4.4024375336398744. Std of Reward: 4.254188375925935.\n",
      "Step: 39000. Mean Reward: 4.7947005399944596. Std of Reward: 4.284904456191608.\n",
      "Step: 40000. Mean Reward: 4.445783574821739. Std of Reward: 4.224963708590592.\n",
      "Step: 41000. Mean Reward: 4.211624115526519. Std of Reward: 4.111701929943491.\n",
      "Step: 42000. Mean Reward: 4.815047044547799. Std of Reward: 4.804339718254001.\n",
      "Step: 43000. Mean Reward: 4.699094725557999. Std of Reward: 4.708078255873119.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 4 : \thazards -> 5.0, hazards_count -> 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 44000. Mean Reward: 5.460237243780558. Std of Reward: 5.262588148956135.\n",
      "Step: 45000. Mean Reward: 4.919371142106378. Std of Reward: 4.395359810454495.\n",
      "Step: 46000. Mean Reward: 4.810800600034839. Std of Reward: 4.350169008601025.\n",
      "Step: 47000. Mean Reward: 5.161270996998619. Std of Reward: 4.686141100394811.\n",
      "Step: 48000. Mean Reward: 4.4819745548914875. Std of Reward: 4.500862729495291.\n",
      "Step: 49000. Mean Reward: 3.776938230717259. Std of Reward: 4.181310211976689.\n",
      "Step: 50000. Mean Reward: 4.147884950306579. Std of Reward: 4.257426989732636.\n",
      "Saved Model\n",
      "Step: 51000. Mean Reward: 4.569018409262139. Std of Reward: 4.092647773717118.\n",
      "Step: 52000. Mean Reward: 4.557228279755218. Std of Reward: 4.497140541723056.\n",
      "Step: 53000. Mean Reward: 5.083598094748259. Std of Reward: 4.9660661638936885.\n",
      "Step: 54000. Mean Reward: 4.896956761783379. Std of Reward: 4.856356671284119.\n",
      "Step: 55000. Mean Reward: 4.352058476430659. Std of Reward: 4.038577495142793.\n",
      "Step: 56000. Mean Reward: 5.014796758959016. Std of Reward: 4.373287119854293.\n",
      "Step: 57000. Mean Reward: 5.263852485132978. Std of Reward: 4.779339218445487.\n",
      "Step: 58000. Mean Reward: 5.424009149744917. Std of Reward: 5.034410357007838.\n",
      "Step: 59000. Mean Reward: 5.586080705239154. Std of Reward: 5.089726074700933.\n",
      "Step: 60000. Mean Reward: 5.660242467394938. Std of Reward: 5.020657109771198.\n",
      "Step: 61000. Mean Reward: 5.788008442050313. Std of Reward: 5.057513203629957.\n",
      "Step: 62000. Mean Reward: 5.507087119956859. Std of Reward: 5.01408170446113.\n",
      "Step: 63000. Mean Reward: 5.191492101355319. Std of Reward: 4.464896595177296.\n",
      "Step: 64000. Mean Reward: 5.556301060355081. Std of Reward: 4.870074397558279.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 5 : \thazards -> 5.0, hazards_count -> 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 65000. Mean Reward: 6.371633719592058. Std of Reward: 5.424866467603259.\n",
      "Step: 66000. Mean Reward: 5.728848874291099. Std of Reward: 5.293688063441778.\n",
      "Step: 67000. Mean Reward: 5.923108409560415. Std of Reward: 5.151138575204245.\n",
      "Step: 68000. Mean Reward: 5.425693870981734. Std of Reward: 5.070574616469505.\n",
      "Step: 69000. Mean Reward: 5.167465371422355. Std of Reward: 5.004238542241832.\n",
      "Step: 70000. Mean Reward: 4.28244033981406. Std of Reward: 4.963513394863455.\n",
      "Step: 71000. Mean Reward: 4.63915489896784. Std of Reward: 4.988766732161587.\n",
      "Step: 72000. Mean Reward: 4.955716339747201. Std of Reward: 5.232641222114726.\n",
      "Step: 73000. Mean Reward: 4.883349907055957. Std of Reward: 4.9962071469041645.\n",
      "Step: 74000. Mean Reward: 5.0541429776247195. Std of Reward: 5.095435204606186.\n",
      "Step: 75000. Mean Reward: 4.817925894644336. Std of Reward: 4.7088146222939.\n",
      "Step: 76000. Mean Reward: 4.350448950712098. Std of Reward: 4.599494494658919.\n",
      "Step: 77000. Mean Reward: 4.6857441705202785. Std of Reward: 5.256452389575775.\n",
      "Step: 78000. Mean Reward: 4.523173498740699. Std of Reward: 5.134241247517766.\n",
      "Step: 79000. Mean Reward: 4.671012684542859. Std of Reward: 5.255942031567225.\n",
      "Step: 80000. Mean Reward: 4.434747819581317. Std of Reward: 5.273296623006816.\n",
      "Step: 81000. Mean Reward: 5.326273317563459. Std of Reward: 5.897206466976277.\n",
      "Step: 82000. Mean Reward: 4.883305019562958. Std of Reward: 5.466795563565693.\n",
      "Step: 83000. Mean Reward: 4.97684383645742. Std of Reward: 5.399164813206343.\n",
      "Step: 84000. Mean Reward: 5.1337376101951175. Std of Reward: 5.133661854348463.\n",
      "Step: 85000. Mean Reward: 4.958222445556778. Std of Reward: 5.403460691552272.\n",
      "Step: 86000. Mean Reward: 5.091484781940078. Std of Reward: 5.5839883821171705.\n",
      "Step: 87000. Mean Reward: 5.722189040610858. Std of Reward: 5.596137098128521.\n",
      "Step: 88000. Mean Reward: 5.670645548388218. Std of Reward: 5.701774675467046.\n",
      "Step: 89000. Mean Reward: 5.46676320376736. Std of Reward: 5.632794595402278.\n",
      "Step: 90000. Mean Reward: 5.645973020879751. Std of Reward: 5.6741427785049305.\n",
      "Step: 91000. Mean Reward: 5.809608319450619. Std of Reward: 5.589261648020787.\n",
      "Step: 92000. Mean Reward: 5.506167041891417. Std of Reward: 5.271819089178817.\n",
      "Step: 93000. Mean Reward: 5.764192869222536. Std of Reward: 5.273973264557908.\n",
      "Step: 94000. Mean Reward: 5.683194387263415. Std of Reward: 5.483142019717235.\n",
      "Step: 95000. Mean Reward: 5.546059029621137. Std of Reward: 5.674688101771941.\n",
      "Step: 96000. Mean Reward: 5.896995790432779. Std of Reward: 5.637351890044767.\n",
      "Step: 97000. Mean Reward: 5.35605872550324. Std of Reward: 5.2784710229526315.\n",
      "Step: 98000. Mean Reward: 5.601410370026458. Std of Reward: 5.55168812606111.\n",
      "Step: 99000. Mean Reward: 5.816024177648358. Std of Reward: 5.868383877156301.\n",
      "Step: 100000. Mean Reward: 6.156937980953391. Std of Reward: 5.732431597083193.\n",
      "Saved Model\n",
      "Step: 101000. Mean Reward: 5.723937252352239. Std of Reward: 5.572871576279706.\n",
      "Step: 102000. Mean Reward: 5.850165780914698. Std of Reward: 5.725853126648321.\n",
      "Step: 103000. Mean Reward: 5.813699909013338. Std of Reward: 5.649913010838355.\n",
      "Step: 104000. Mean Reward: 5.938527515950518. Std of Reward: 6.070246720549026.\n",
      "Step: 105000. Mean Reward: 5.7196717021507455. Std of Reward: 5.727982112557697.\n",
      "Step: 106000. Mean Reward: 5.8069887148270585. Std of Reward: 5.602798984389656.\n",
      "Step: 107000. Mean Reward: 6.066425783789338. Std of Reward: 5.682206344640763.\n",
      "Step: 108000. Mean Reward: 5.737301072382718. Std of Reward: 5.5875920131323475.\n",
      "Step: 109000. Mean Reward: 5.831839284901638. Std of Reward: 5.887318677224457.\n",
      "Step: 110000. Mean Reward: 5.359218312639098. Std of Reward: 5.98083664783891.\n",
      "Step: 111000. Mean Reward: 5.683124544978986. Std of Reward: 6.067750244505103.\n",
      "Step: 112000. Mean Reward: 5.286562265553579. Std of Reward: 5.796300602465867.\n",
      "Step: 113000. Mean Reward: 5.7216518090717186. Std of Reward: 6.043500124846701.\n",
      "Step: 114000. Mean Reward: 5.592207977674758. Std of Reward: 5.856357433433051.\n",
      "Step: 115000. Mean Reward: 5.54613469151472. Std of Reward: 5.896648901323112.\n",
      "Step: 116000. Mean Reward: 5.595908052323133. Std of Reward: 5.769563795191229.\n",
      "Step: 117000. Mean Reward: 5.487674559511099. Std of Reward: 5.990447705332966.\n",
      "Step: 118000. Mean Reward: 5.305143118218087. Std of Reward: 5.996750612564171.\n",
      "Step: 119000. Mean Reward: 4.9968351793006995. Std of Reward: 5.867462430722341.\n",
      "Step: 120000. Mean Reward: 5.285555089984895. Std of Reward: 5.823894728205134.\n",
      "Step: 121000. Mean Reward: 5.779667604358196. Std of Reward: 6.081781737538292.\n",
      "Step: 122000. Mean Reward: 5.621773448449778. Std of Reward: 5.832068520854407.\n",
      "Step: 123000. Mean Reward: 5.262418538591374. Std of Reward: 5.789629871898987.\n",
      "Step: 124000. Mean Reward: 5.785987375477617. Std of Reward: 6.000447788812677.\n",
      "Step: 125000. Mean Reward: 5.640180484405859. Std of Reward: 6.382690744769008.\n",
      "Step: 126000. Mean Reward: 5.834161903076598. Std of Reward: 6.338755657096296.\n",
      "Step: 127000. Mean Reward: 6.071117182206258. Std of Reward: 6.425104837097373.\n",
      "Step: 128000. Mean Reward: 6.006780445056499. Std of Reward: 6.37373069853793.\n",
      "Step: 129000. Mean Reward: 5.482374558765153. Std of Reward: 6.154418763749823.\n",
      "Step: 130000. Mean Reward: 6.206603794877477. Std of Reward: 6.330797117457644.\n",
      "Step: 131000. Mean Reward: 5.990746516673997. Std of Reward: 6.374832073602006.\n",
      "Step: 132000. Mean Reward: 6.223905142625658. Std of Reward: 6.064057650433626.\n",
      "Step: 133000. Mean Reward: 6.149732915696551. Std of Reward: 6.0063668666617716.\n",
      "Step: 134000. Mean Reward: 6.039054064612506. Std of Reward: 6.267322315057839.\n",
      "Step: 135000. Mean Reward: 5.526001717547439. Std of Reward: 5.884575028702403.\n",
      "Step: 136000. Mean Reward: 5.828217464781939. Std of Reward: 6.4928065004036934.\n",
      "Step: 137000. Mean Reward: 5.789647818429177. Std of Reward: 6.3152489202193784.\n",
      "Step: 138000. Mean Reward: 6.106396565153799. Std of Reward: 6.339168873473954.\n",
      "Step: 139000. Mean Reward: 6.625652376914917. Std of Reward: 6.419198200594803.\n",
      "Step: 140000. Mean Reward: 5.934395517811719. Std of Reward: 6.031996529056787.\n",
      "Step: 141000. Mean Reward: 6.1455295233913185. Std of Reward: 6.478871882388508.\n",
      "Step: 142000. Mean Reward: 5.966812518932539. Std of Reward: 6.288881614461777.\n",
      "Step: 143000. Mean Reward: 6.041481331532399. Std of Reward: 6.523498704251125.\n",
      "Step: 144000. Mean Reward: 6.099921601563735. Std of Reward: 6.0274031988386145.\n",
      "Step: 145000. Mean Reward: 5.988523266938029. Std of Reward: 6.209504416700278.\n",
      "Step: 146000. Mean Reward: 5.520237037529358. Std of Reward: 6.268447895379057.\n",
      "Step: 147000. Mean Reward: 5.502486907119159. Std of Reward: 6.12230046832264.\n",
      "Step: 148000. Mean Reward: 5.3117797128501. Std of Reward: 5.817833721076537.\n",
      "Step: 149000. Mean Reward: 5.713621121581516. Std of Reward: 6.092841906413921.\n",
      "Step: 150000. Mean Reward: 5.940391240518032. Std of Reward: 6.23594489218804.\n",
      "Saved Model\n",
      "Saved Model\n",
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-150001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-150001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-150001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-150001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
